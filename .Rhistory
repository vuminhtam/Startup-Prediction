#Source code: https://www.r-bloggers.com/using-knn-classifier-to-predict-whether-the-price-of-stock-will-increase/
accuracy <- rep(0, 11-5+1) #initialize with 0 accuracy
k <- 5:11
for(x in k){
prediction <- knn(train_Set, test_Set, train_Set$Glass.Type, k = x)
accuracy[x] <- mean(prediction == test_Set$Glass.Type) * 100 #calculate accuracy
}
#What is the optimal k, i.e., the k that results in the best accuracy? Plot k versus accuracy.
plot(k, accuracy[5:11], type = 'b', main = "Accuracy by k", xlab = "accuracy (%)")
#12. Create a plot of k (x-axis) versus error rate (percentage of incorrect classifications).
error <- 1-accuracy
plot(k, error[5:11], type = 'b', main = "Error rate by k", xlab = "error (%)")
#13. Produce a cross-table confusion matrix showing the accuracy of the classification using a package of your choice and k = 5
prediction <- class::knn(train_Set, test_Set, train_Set$Glass.Type, k = 6)
table(prediction, test_Set$Glass.Type)
#% accurate prediction
mean(prediction == test_Set$Glass.Type) * 100
#14. Comment on the run-time complexity of the k-NN for classifying w new cases using a training data set of n cases having m features.
#for each new case in w, compute distance with m feature n times in training data set --> O(wmn)
# Assume that m is "large". How does this algorithm behave as w, n, and m increase?
# the algorithm increases exponentially as m,n,w increases. if w,n,m are very large, algorithm is not "fast"
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df$RI)
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df[2])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df[2])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df[2,])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df[,2])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
hist(df[,3])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
df
hist(df[,4])
hist(df[,5])
hist(df[,6])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
df
hist(df[,6])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers? Si, Mg
df
hist(df[,7])
hist(df[,8])
hist(df[,9])
hist(df[,10])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers? Si, Mg,
df
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers? Si, Mg, Fe,
df[8]
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers? Si, Mg, Fe,
hist(df[8])
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers? Si, Mg, Fe,
hist(df[,8])
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, )
}
df[,3]
data.frame(outlier_values)
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, length(outliers))
}
#how many and which columns were removed
col_removed <- subset(df, df$Na %in% outliers$outlier_values)$ID
col_removed
#...
outliers_count
outliers_count
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers for each column ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, length(outliers))
}
outliers_count
#how many and which columns were removed
col_removed <- subset(df, df$Na %in% outliers$outlier_values)$ID
col_removed
#...
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers for each column ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, nrow(outliers))
}
outliers_count
df
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers for each column ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, nrow(outliers))
}
outliers_count
#no column need to be removed because number of outliers are not significant relatively
#no column need to be removed because number of outliers are not significant relatively
df
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers for each column ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, nrow(outliers))
}
outliers_count
#no column need to be removed because number of outliers are not significant relatively
df
#5. remove ID column
df <- df[-1]
# normalize the first two columns in the data set using min-max normalization
min_max_norm <- function(data) {
options(scipen = 999)
minX <- min(as.numeric(data))
maxX <- max(as.numeric(data))
data <- (as.numeric(data) - minX)/(maxX - minX)
}
min_max_norm(df$RI)
min_max_norm(df$Na)
df
#1. Read input, create header names
url <- "/Users/tamvu/Downloads/glass.data.txt"
df <- read.table(url, header=F, sep=",", na.strings = c("", "NA"))
colnames(df) <- c("ID", "RI", "Na", "Mg", "Al", "Si", "K",
"Ca", "Ba", "Fe", "Glass.Type")
df
#2. What are the distributions? Is there skewness? Do you need to transform the data? Are there outliers?
#Answer: Data of Si, Mg, Fe are not normally distributed. There are outliers
#However for knn analysis we do not need to transform the data.
#3. Create a histogram of the Na column and overlay a normal curve;
#source code: https://www.statmethods.net/graphs/density.html
x <- df$Na
h <- hist(x, main="Histogram of Na", xlab="Na")
#add normal curve
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
#visually data is not normally distributed. slighted skewed right
#k-NN algorithm does not require normally distributed data
#k-NN algorithm is non-parametric
#4. Identify outliers
#Univariate approach
#used boxplot(data)$out that applies Tukey’s method to identify the outliers for each column ranged above and below the 1.5*IQR
#source code: http://r-statistics.co/Outlier-Treatment-With-R.html
outliers_count <- c()
for(i in 1:length(df)) {
outlier_values <- boxplot.stats(df[,i])$out
outliers <- data.frame(outlier_values)
outliers_count <- c(outliers_count, nrow(outliers))
}
outliers_count
#no column need to be removed because number of outliers are not significant relatively
df
#5. remove ID column
df <- df[-1]
# normalize the first two columns in the data set using min-max normalization
min_max_norm <- function(data) {
options(scipen = 999)
minX <- min(as.numeric(data))
maxX <- max(as.numeric(data))
data <- (as.numeric(data) - minX)/(maxX - minX)
}
min_max_norm(df$RI)
min_max_norm(df$Na)
df
#6. Normalize the remaining columns, except the last one, using z-score standardization.
#The last column is the glass type and so it is excluded.
for(i in 3:(length(df)-1)) {
std_dev <- sd(df[,i])
mean <- mean(df[,i])
df[,i] <- (df[,i] - mean)/std_dev
}
df
#7. The data set is sorted, so creating a validation data set requires random selection of elements.
#Create a stratified sample where you randomly select 50% of each of the cases for each glass type to be part of the validation data set.
train_Set <- data.frame()
set.seed(5)
for(i in 1:7) {
all_of_type_i <- subset(df, df$Glass.Type == i)
train_Set <- rbind(train_Set, all_of_type_i[sample(0.5 * nrow(all_of_type_i)),])
}
#The remaining cases will form the training data set.
test_Set <- df[!(as.numeric(row.names(df)) %in% as.numeric(row.names(train_Set))), ]
train_Set
#8. Implement the k-NN algorithm in R (do not use an implementation of k-NN from a package) and use your algorithm with a k=10 to predict the glass type for the following two cases:
k <- 10
#source code: http://dataaspirant.com/2017/01/02/k-nearest-neighbor-classifier-implementation-r-scratch/
#calculate distance between 2 entries
euclideanDist <- function(a, b){
d = 0
for(i in c(1:(length(a)-1) ))
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
case1 <- c(1.51621 , 12.53 , 3.48 , 1.39 , 73.39 , 0.60 , 8.55 , 0.00 , 0.05)
case2 <- c(1.5098 , 12.77 , 1.85 , 1.81 , 72.69 , 0.59 , 10.01 , 0.00 , 0.01)
#calculate the eu distance btw each item and case
knn_me <- function(case, k) {
dist <- c()
for(item in 1:nrow(train_Set)) {
dist <- c(dist, euclideanDist(train_Set[item, ], case))
}
#add dist column
train_Set$dist <- dist
#sort the training data based on eu dist
train_Set <- train_Set[order(train_Set$dist),]
return(which.max(as.data.frame(table(train_Set[1:k]$Glass.Type))$Var1))
}
knn_me(case1, k)
knn_me(case2, k)
two_tests <- data.frame(rbind(case1, case2))
library('class')
#9.  Apply the knn function from the class package with k=11 and redo the cases
knn(train = train_Set[-10], test = case1, train_Set$Glass.Type, k = 11)
knn(train = train_Set[-10], test = case2, train_Set$Glass.Type, k = 11)
#10. knn function with k=11 from the class package
#by applying it against each case in the validation data set.
pred_validation <- class::knn(train = train_Set,
test = test_Set,
train_Set$Glass.Type,
k = 11)
table(pred_validation, test_Set$Glass.Type)
# What is the percentage of correct classifications?
accuracy <- mean(pred_validation == test_Set$Glass.Type) * 100
accuracy
#11. Determine an optimal k by trying all values from 5 through 11 for your own k-NN algorithm implementation against the cases in the validation data set.
#Source code: https://www.r-bloggers.com/using-knn-classifier-to-predict-whether-the-price-of-stock-will-increase/
accuracy <- rep(0, 11-5+1) #initialize with 0 accuracy
k <- 5:11
for(x in k){
prediction <- knn(train_Set, test_Set, train_Set$Glass.Type, k = x)
accuracy[x] <- mean(prediction == test_Set$Glass.Type) * 100 #calculate accuracy
}
#What is the optimal k, i.e., the k that results in the best accuracy? Plot k versus accuracy.
plot(k, accuracy[5:11], type = 'b', main = "Accuracy by k", xlab = "accuracy (%)")
#12. Create a plot of k (x-axis) versus error rate (percentage of incorrect classifications).
error <- 1-accuracy
plot(k, error[5:11], type = 'b', main = "Error rate by k", xlab = "error (%)")
#13. Produce a cross-table confusion matrix showing the accuracy of the classification using a package of your choice and k = 5
prediction <- class::knn(train_Set, test_Set, train_Set$Glass.Type, k = 6)
table(prediction, test_Set$Glass.Type)
#% accurate prediction
mean(prediction == test_Set$Glass.Type) * 100
#14. Comment on the run-time complexity of the k-NN for classifying w new cases using a training data set of n cases having m features.
#for each new case in w, compute distance with m feature n times in training data set --> O(wmn)
# Assume that m is "large". How does this algorithm behave as w, n, and m increase?
# the algorithm increases exponentially as m,n,w increases. if w,n,m are very large, algorithm is not "fast"
sd(cars)
sd(as.numeric(cars))
sd(as.numeric(cars$dist))
(83-mean(cars$dist))/sd(as.numeric(cars$dist))
df <- read.xlsx(url, 1, startRow = 1)
library('openxlsx')
url <- "/Users/tamvu/Downloads/HealthDataSet.xlsx"
df <- read.xlsx(url, 1, startRow = 1)
lm(X1 ~., data = df)
df
df <- read.xlsx(url)
df
df <- read.xlsx(url, 2)
df
df <- as.data.frome(read.xlsx(url, 2))
df <- as.data.frame(read.xlsx(url, 2))
df
lm(X1 ~., data = df)
summary(lm(X1 ~., data = df))
#Import data set from url: https://www.kaggle.com/kemical/kickstarter-projects/data
url1 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201801.csv"
url2 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201612.csv"
df <- rbind(as.data.frame(read.csv(url1)), as.data.frame(read.csv(url2)))
df <- rbind(as.data.frame(read.csv(url1))[1:13], as.data.frame(read.csv(url2)))
df <- rbind(as.data.frame(read.csv(url1)), as.data.frame(read.csv(url2))[1:13])
#Import data set from url: https://www.kaggle.com/kemical/kickstarter-projects/data
url1 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201801.csv"
url2 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201612.csv"
df <- rbind(as.data.frame(read.csv(url1))[1:13], as.data.frame(read.csv(url2))[1:13])
#UNDERSTAND THE BUSINESS
#It is important for entrepreneurs to know how likely a project is going to be "sucessful" if launched on Kickstarter
#possible factors include not meeting minimum funding goal or the category of the product/service.
#This project analyzes statistics to find correlation among 13 features of a project
#to predict a likelihood of a project to be succesful on Kickstarter - that is fully funded
#Import data set from url: https://www.kaggle.com/kemical/kickstarter-projects/data
#data in 2018
url1 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201801.csv"
raw_df <- as.data.frame(read.csv(url1), na.strings = c("", "NA"))
#UNDERSTAND THE BUSINESS
#It is important for entrepreneurs to know how likely a project is going to be "sucessful" if launched on Kickstarter
#possible factors include not meeting minimum funding goal or the category of the product/service.
#This project analyzes statistics to find correlation among 13 features of a project
#to predict a likelihood of a project to be succesful on Kickstarter - that is fully funded
#Import data set from url: https://www.kaggle.com/kemical/kickstarter-projects/data
#data in 2018
url1 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201801.csv"
raw_df <- as.data.frame(read.csv(url1), na.strings = c("", "NA"))
#UNDERSTAND THE DATA
#There are 15 variables
#Since ID and names are indentity of the data rather than a factor that determines any outcome,
#one of the variable is the actual outcome of the data: sucessful/failed/canceled/live.
#the rest are features that can contribute to the state
#DEFINE OUTCOME "sucessful":
#states of a project recorded into the dataset
df <- raw_df
table(df$state)
#I am only taking into account projects that are not live or suspended (because there is no result to it yet if so)
#projects that are not canceled (because the result is not one of the features in the dataset)
#only projects that is determined with final state (either failed/sucessful)
#DATA PREPARATION: CLEANING, IMPUTATION
#raw data has 378661 entries.
#MISSING VALUES:
#check if any variables that have too much missing values
library(Amelia)
missmap(df, main = "Missing values")
#according to the draft of missing values there is no variable that has significant missing values
#so disregard all entries that have any missing values of any variable
#which results in 374864 entries with total 15 features
df <- na.omit(df)
#FILTER DATA to sucessful/failed only. results in 331465 entries
df <- subset(df, df$state == "failed" | df$state == "successful")
#OMIT IRRELEVANT variables: ID and names. results in 13 variables: 12 features and 1 that is the state
nonvars <- c("ID","name")
df <- df[,!(names(df) %in% nonvars)]
#FILTER DATA to sucessful/failed only. results in 331465 entries
df <- subset(df, df$state == "failed" | df$state == "successful")
#OMIT IRRELEVANT variables: ID and names. results in 13 variables: 12 features and 1 that is the state
nonvars <- c("ID","name")
df <- df[,!(names(df) %in% nonvars)]
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- table(df$state) / nrow(df) * 100
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
main="Pie Chart of Sucessful projects")
table(df$state)
as.data.frame(table(df$state))
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state)) / nrow(df) * 100
mytable
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- table(df$state) / nrow(df) * 100
mytable <- as.data.frame(mytable)
mytable
raw_df <- as.data.frame(read.csv(url1), na.strings = c("", "NA", "undefined"))
#Import data set from url: https://www.kaggle.com/kemical/kickstarter-projects/data
#data in 2018
url1 <- "/Users/tamvu/DS4100/Kickstarter Predict/Startup Prediction/kickstarter-projects/ks-projects-201801.csv"
raw_df <- as.data.frame(read.csv(url1), na.strings = c("", "NA", "undefined"))
raw_df <- as.data.frame(read.csv(url1), na.strings = c("", "NA", "undefined"))
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable <- subset(mytable, mytable$Var1 == "failed" | mytable$Var1 == "successful")
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
main="Pie Chart of Sucessful projects")
mytable <- subset(mytable, mytable$Var1 == "failed" | mytable$Var1 == "successful")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable <- subset(mytable, mytable$Var1 == "failed" | mytable$Var1 == "successful")
mytable
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
main="Pie Chart of Sucessful projects")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
#mytable <- subset(mytable, mytable$Var1 == "failed" | mytable$Var1 == "successful")
lbls <- paste(names(mytable), "\n", mytable, sep="")
pie(mytable, labels = lbls,
main="Pie Chart of Sucessful projects")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
slices <- c(59.61836, 40.38164)
lbls <- c( "failed", "successful")
pie(slices, labels = lbls,
main="Pie Chart of Sucessful projects")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable
slices <- c(59.62, 40.38)
lbls <- c( "failed", "successful")
pie(slices, labels = lbls,
main="Pie Chart of Sucessful projects")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable
slices <- c(59.62, 40.38)
lbls <- paste(c( "failed", "successful"), slices)
pie(slices, labels = lbls, main="Pie Chart of Sucessful projects")
#EXPLORATORY DATA ANALYSIS: analyze to find important factors that indicate the outcome of successful or failed projects
#percentage of sucessful vs failed
mytable <- as.data.frame(table(df$state) / nrow(df) * 100)
mytable
slices <- c(59.62, 40.38)
lbls <- paste(c( "failed", "successful"), slices, "%")
pie(slices, labels = lbls, main="Pie Chart of Sucessful projects")
names(df)
hist(df$backers)
df
df$backers
hist(df$backers)
hist(df$backers, 1)
hist(df$backers, 2)
df$backers
hist(df$pledged)
hist(df$category)
hist(df$pledged)
max(df$pledged)
hist(df$pledged)
?hist
plot(df$pledged)
plot(df$backers)
x <- df$backers
h <- hist(x, main="Histogram of backers", xlab="number of backers")
#add normal curve
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
table(df$backers)
as.data.frame(table(df$backers))
hist(as.data.frame(table(df$backers)))
hist(table(df$backers))
x <- df$backers
h <- hist(x, main="Histogram of backers", xlab="number of backers")
#add normal curve
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)
sd(df$backers)
outliers::outlier(df$backers)
outliers::rm.outlier(df$backers)
h <- hist(outliers::rm.outlier(df$backers), main="Histogram of backers", xlab="number of backers")
outlierKD <- function(dt, var) {
var_name <- eval(substitute(var),eval(dt))
tot <- sum(!is.na(var_name))
na1 <- sum(is.na(var_name))
m1 <- mean(var_name, na.rm = T)
par(mfrow=c(2, 2), oma=c(0,0,3,0))
boxplot(var_name, main="With outliers")
hist(var_name, main="With outliers", xlab=NA, ylab=NA)
outlier <- boxplot.stats(var_name)$out
mo <- mean(outlier)
var_name <- ifelse(var_name %in% outlier, NA, var_name)
boxplot(var_name, main="Without outliers")
hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
title("Outlier Check", outer=TRUE)
na2 <- sum(is.na(var_name))
message("Outliers identified: ", na2 - na1, " from ", tot, " observations")
message("Proportion (%) of outliers: ", (na2 - na1) / tot*100)
message("Mean of the outliers: ", mo)
m2 <- mean(var_name, na.rm = T)
message("Mean without removing outliers: ", m1)
message("Mean if we remove outliers: ", m2)
response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
if(response == "y" | response == "yes"){
dt[as.character(substitute(var))] <- invisible(var_name)
assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
message("Outliers successfully removed", "\n")
return(invisible(dt))
} else{
message("Nothing changed", "\n")
return(invisible(var_name))
}
}
outlierKD(df,backers)
